# src/core/model_analysis_recommendation.py
"""
An치lisis Comparativo y Recomendaci칩n de Modelos para Timeframe 1min.

Este script analiza las caracter칤sticas te칩ricas de cada modelo disponible
y genera recomendaciones iniciales optimizadas para producci칩n con timeframe 1min.

Factores considerados:
- Tama침o del modelo (par치metros) - afecta latencia
- Arquitectura (MoE vs dense) - afecta throughput
- Especializaci칩n (coder, vision, reasoning)
- Benchmarks conocidos (si disponibles)
- Requisitos del agente (t칠cnico, sentimiento, decisi칩n, etc.)

Modelos disponibles en Ollama Cloud:
1.  rnj-1:8b-cloud              - 8B, r치pido, gen칠rico
2.  ministral-3:14b-cloud      - 14B, Mistral, buen balance
3.  devstral-small-2:24b-cloud - 24B, razonamiento c칩digo
4.  nemotron-3-nano:30b-cloud  - 30B, NVIDIA, t칠cnico
5.  kimi-k2.5:cloud            - Unknown, Moonshot AI
6.  kimi-k2-thinking:cloud     - Thinking model
7.  deepseek-v3.2:cloud        - DeepSeek, buen balance
8.  minimax-m2.1:cloud         - MiniMax, decisiones
9.  glm-4.7:cloud              - Zhipu, chino-ingl칠s
10. deepseek-v3.1:671b-cloud   - 671B MoE, muy lento
11. gpt-oss:120b-cloud         - 120B, OpenAI-style
12. qwen3-coder:480b-cloud     - 480B, coder especializado
13. qwen3-next:80b-cloud       - 80B, gran capacidad
14. gemini-3-flash-preview     - Vision + Flash = r치pido
"""
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Any
from datetime import datetime


@dataclass
class ModelProfile:
    """Perfil de un modelo LLM."""
    name: str
    params_b: float  # Par치metros en billones
    architecture: str  # dense, moe, unknown
    specialization: list[str]  # coding, vision, reasoning, chat, technical
    expected_latency_1k_tokens: float  # ms (estimado)
    throughput_tokens_per_sec: float  # estimado
    best_for: list[str]  # Agentes recomendados
    not_recommended_for: list[str]
    score_1min_tf: float  # 0-100, optimizado para 1min


# Perfiles de modelos basados en especificaciones conocidas
MODEL_PROFILES: dict[str, ModelProfile] = {
    "rnj-1:8b-cloud": ModelProfile(
        name="rnj-1:8b-cloud",
        params_b=8,
        architecture="dense",
        specialization=["chat", "fast"],
        expected_latency_1k_tokens=800,  # ~0.8s para 1k tokens
        throughput_tokens_per_sec=1200,
        best_for=["sentiment", "qabba"],  # Tareas simples, r치pidas
        not_recommended_for=["decision", "risk_manager"],  # Requiere m치s reasoning
        score_1min_tf=85,  # Muy r치pido, bueno para 1min
    ),

    "ministral-3:14b-cloud": ModelProfile(
        name="ministral-3:14b-cloud",
        params_b=14,
        architecture="dense",
        specialization=["chat", "instruct", "technical"],
        expected_latency_1k_tokens=1200,  # ~1.2s
        throughput_tokens_per_sec=800,
        best_for=["technical", "sentiment", "qabba"],
        not_recommended_for=[],
        score_1min_tf=92,  # Excelente balance velocidad/calidad
    ),

    "devstral-small-2:24b-cloud": ModelProfile(
        name="devstral-small-2:24b-cloud",
        params_b=24,
        architecture="dense",
        specialization=["coding", "reasoning", "technical"],
        expected_latency_1k_tokens=2000,  # ~2s
        throughput_tokens_per_sec=500,
        best_for=["decision", "risk_manager", "technical"],
        not_recommended_for=["sentiment"],  # Overkill para sentimiento
        score_1min_tf=88,  # Bueno para reasoning, pero m치s lento
    ),

    "nemotron-3-nano:30b-cloud": ModelProfile(
        name="nemotron-3-nano:30b-cloud",
        params_b=30,
        architecture="dense",
        specialization=["technical", "instruct", "chat"],
        expected_latency_1k_tokens=2500,  # ~2.5s
        throughput_tokens_per_sec=400,
        best_for=["technical", "qabba", "decision"],
        not_recommended_for=["sentiment"],
        score_1min_tf=85,
    ),

    "kimi-k2.5:cloud": ModelProfile(
        name="kimi-k2.5:cloud",
        params_b=32,  # Estimado
        architecture="dense",
        specialization=["chat", "long_context", "reasoning"],
        expected_latency_1k_tokens=2800,
        throughput_tokens_per_sec=350,
        best_for=["decision", "risk_manager"],
        not_recommended_for=["sentiment", "technical"],
        score_1min_tf=82,
    ),

    "kimi-k2-thinking:cloud": ModelProfile(
        name="kimi-k2-thinking:cloud",
        params_b=32,  # Estimado
        architecture="dense",
        specialization=["reasoning", "coding", "thinking"],
        expected_latency_1k_tokens=4000,  # M치s lento por thinking
        throughput_tokens_per_sec=250,
        best_for=["decision"],  # Thinking es bueno para decisiones
        not_recommended_for=["technical", "sentiment", "qabba", "risk_manager"],
        score_1min_tf=70,  # Thinking es lento para 1min
    ),

    "deepseek-v3.2:cloud": ModelProfile(
        name="deepseek-v3.2:cloud",
        params_b=16,  # Estimado (version peque침a)
        architecture="dense",
        specialization=["coding", "technical", "chat"],
        expected_latency_1k_tokens=1500,
        throughput_tokens_per_sec=650,
        best_for=["technical", "qabba", "sentiment"],
        not_recommended_for=[],
        score_1min_tf=90,
    ),

    "minimax-m2.1:cloud": ModelProfile(
        name="minimax-m2.1:cloud",
        params_b=24,  # Estimado
        architecture="dense",
        specialization=["chat", "decision", "reasoning"],
        expected_latency_1k_tokens=2200,
        throughput_tokens_per_sec=450,
        best_for=["decision", "risk_manager"],
        not_recommended_for=["sentiment"],
        score_1min_tf=86,
    ),

    "glm-4.7:cloud": ModelProfile(
        name="glm-4.7:cloud",
        params_b=9,  # Estimado
        architecture="dense",
        specialization=["chat", "multilingual"],
        expected_latency_1k_tokens=1000,
        throughput_tokens_per_sec=900,
        best_for=["sentiment"],  # Bueno para an치lisis de texto
        not_recommended_for=["technical", "decision"],
        score_1min_tf=88,
    ),

    # Modelos GRANDES (lentos, no recomendados para 1min pero 칰tiles para referencia)
    "deepseek-v3.1:671b-cloud": ModelProfile(
        name="deepseek-v3.1:671b-cloud",
        params_b=671,
        architecture="moe",
        specialization=["coding", "reasoning", "technical"],
        expected_latency_1k_tokens=8000,  # ~8s muy lento
        throughput_tokens_per_sec=120,
        best_for=[],  # No recomendado para 1min
        not_recommended_for=["technical", "sentiment", "qabba", "decision", "risk_manager", "visual"],
        score_1min_tf=30,  # Demasiado lento para 1min
    ),

    "gpt-oss:120b-cloud": ModelProfile(
        name="gpt-oss:120b-cloud",
        params_b=120,
        architecture="dense",
        specialization=["chat", "general"],
        expected_latency_1k_tokens=5000,
        throughput_tokens_per_sec=200,
        best_for=[],  # Lento
        not_recommended_for=["technical", "sentiment", "qabba", "decision", "risk_manager"],
        score_1min_tf=45,
    ),

    "qwen3-coder:480b-cloud": ModelProfile(
        name="qwen3-coder:480b-cloud",
        params_b=480,
        architecture="dense",
        specialization=["coding", "technical"],
        expected_latency_1k_tokens=6000,
        throughput_tokens_per_sec=160,
        best_for=[],  # Especializado en c칩digo, no trading
        not_recommended_for=["technical", "sentiment", "qabba", "decision", "risk_manager", "visual"],
        score_1min_tf=35,
    ),

    "qwen3-next:80b-cloud": ModelProfile(
        name="qwen3-next:80b-cloud",
        params_b=80,
        architecture="dense",
        specialization=["chat", "reasoning", "general"],
        expected_latency_1k_tokens=3500,
        throughput_tokens_per_sec=280,
        best_for=["decision"],  # Solo si el pipeline es r치pido
        not_recommended_for=["technical", "sentiment", "qabba"],
        score_1min_tf=65,
    ),

    # Vision
    "gemini-3-flash-preview:cloud": ModelProfile(
        name="gemini-3-flash-preview:cloud",
        params_b=8,  # Estimado para versi칩n flash
        architecture="dense",
        specialization=["vision", "fast", "multimodal"],
        expected_latency_1k_tokens=1500,  # Vision es m치s lento
        throughput_tokens_per_sec=600,
        best_for=["visual"],  # Especializado en visi칩n
        not_recommended_for=["technical", "sentiment", "qabba", "decision", "risk_manager"],
        score_1min_tf=90,  # Flash = r치pido incluso con vision
    ),
}


# Requisitos por tipo de agente
AGENT_REQUIREMENTS: dict[str, dict[str, Any]] = {
    "technical": {
        "description": "An치lisis t칠cnico de indicadores",
        "needs": ["technical", "instruct"],
        "speed_priority": "high",  # Necesita ser r치pido
        "reasoning_priority": "medium",
        "json_precision": "high",  # JSON debe ser preciso
        "max_latency_ms": 5000,  # < 5s
    },
    "sentiment": {
        "description": "An치lisis de sentimiento de noticias/social",
        "needs": ["chat", "multilingual"],
        "speed_priority": "high",
        "reasoning_priority": "low",
        "json_precision": "medium",
        "max_latency_ms": 4000,  # < 4s
    },
    "qabba": {
        "description": "An치lisis de microestructura (OBI, CVD)",
        "needs": ["technical", "coding"],  # An치lisis num칠rico
        "speed_priority": "high",
        "reasoning_priority": "medium",
        "json_precision": "high",
        "max_latency_ms": 5000,
    },
    "visual": {
        "description": "An치lisis visual de gr치ficos",
        "needs": ["vision"],
        "speed_priority": "medium",
        "reasoning_priority": "medium",
        "json_precision": "high",
        "max_latency_ms": 8000,  # Vision es m치s lento
    },
    "decision": {
        "description": "S칤ntesis final y decisi칩n de trading",
        "needs": ["reasoning", "decision"],
        "speed_priority": "medium",  # Puede ser m치s lento
        "reasoning_priority": "high",  # Necesita buen razonamiento
        "json_precision": "high",
        "max_latency_ms": 10000,  # < 10s
    },
    "risk_manager": {
        "description": "Evaluaci칩n de riesgo y gesti칩n",
        "needs": ["reasoning", "technical"],
        "speed_priority": "medium",
        "reasoning_priority": "high",
        "json_precision": "high",
        "max_latency_ms": 8000,
    },
}


def score_model_for_agent(model: ModelProfile, agent: str) -> tuple[float, str]:
    """
    Calcula un score de compatibilidad entre modelo y agente.

    Returns:
        (score, reasoning)
    """
    reqs = AGENT_REQUIREMENTS.get(agent, {})
    if not reqs:
        return 0, "Unknown agent"

    # Verificar si est치 en not_recommended_for
    if agent in model.not_recommended_for:
        return 20, f"Modelo no recomendado para {agent}"

    score = 0.0
    reasons = []

    # 1. Especializaci칩n (30%)
    specialization_match = 0
    for need in reqs["needs"]:
        if need in model.specialization:
            specialization_match += 1
    spec_score = (specialization_match / len(reqs["needs"])) * 30
    score += spec_score
    reasons.append(f"Especializaci칩n: {spec_score:.0f}/30")

    # 2. Velocidad (40% para 1min)
    max_acceptable = reqs["max_latency_ms"]
    if model.expected_latency_1k_tokens <= max_acceptable * 0.5:
        speed_score = 40  # Excelente
    elif model.expected_latency_1k_tokens <= max_acceptable:
        speed_score = 30  # Bueno
    elif model.expected_latency_1k_tokens <= max_acceptable * 1.5:
        speed_score = 20  # Aceptable
    else:
        speed_score = 10  # Lento
    score += speed_score
    reasons.append(f"Velocidad: {speed_score}/40 (latencia: {model.expected_latency_1k_tokens:.0f}ms)")

    # 3. Score general del modelo para 1min (20%)
    model_score = (model.score_1min_tf / 100) * 20
    score += model_score
    reasons.append(f"Score 1min: {model_score:.0f}/20")

    # 4. Throughput (10%)
    if model.throughput_tokens_per_sec >= 800:
        throughput_score = 10
    elif model.throughput_tokens_per_sec >= 500:
        throughput_score = 7
    elif model.throughput_tokens_per_sec >= 300:
        throughput_score = 5
    else:
        throughput_score = 3
    score += throughput_score
    reasons.append(f"Throughput: {throughput_score}/10")

    return score, " | ".join(reasons)


def generate_recommendations() -> dict[str, Any]:
    """Genera recomendaciones para cada agente."""
    recommendations = {}

    for agent, reqs in AGENT_REQUIREMENTS.items():
        agent_recs = []

        for model_name, profile in MODEL_PROFILES.items():
            score, reasoning = score_model_for_agent(profile, agent)
            agent_recs.append({
                "model": model_name,
                "score": score,
                "reasoning": reasoning,
                "params_b": profile.params_b,
                "expected_latency_ms": profile.expected_latency_1k_tokens,
            })

        # Ordenar por score
        agent_recs.sort(key=lambda x: x["score"], reverse=True)

        recommendations[agent] = {
            "description": reqs["description"],
            "requirements": reqs["needs"],
            "top_3_models": agent_recs[:3],
            "recommended": agent_recs[0] if agent_recs else None,
        }

    return recommendations


def generate_yaml_config(recommendations: dict) -> str:
    """Genera configuraci칩n YAML basada en recomendaciones."""
    lines = [
        "# config/llm_providers.yaml",
        "# Configuraci칩n optimizada para timeframe 1min basada en an치lisis de modelos",
        f"# Generado: {datetime.now().isoformat()}",
        "#",
        "# AN츼LISIS:",
        "# - Prioridad: Velocidad > Razonamiento para 1min",
        "# - Modelos grandes (>80B) excluidos por latencia",
        "# - Especializaci칩n por agente seg칰n requisitos",
        "",
        "active_profile: 'ollama_1min_optimized'",
        "",
        "ollama_1min_optimized:",
    ]

    agent_yaml_names = {
        "technical": "technical",
        "sentiment": "sentiment",
        "qabba": "qabba",
        "visual": "visual",
        "decision": "decision",
        "risk_manager": "risk_manager",
    }

    for agent, rec in recommendations.items():
        if not rec["recommended"]:
            continue

        model = rec["recommended"]["model"]
        score = rec["recommended"]["score"]

        # Determinar temperature seg칰n agente
        if agent == "technical":
            temp = 0.05
            max_tokens = 2500
            timeout = 8
        elif agent == "sentiment":
            temp = 0.15
            max_tokens = 1500
            timeout = 6
        elif agent == "qabba":
            temp = 0.05
            max_tokens = 1200
            timeout = 7
        elif agent == "visual":
            temp = 0.05
            max_tokens = 1500
            timeout = 10
        elif agent == "decision":
            temp = 0.1
            max_tokens = 2000
            timeout = 12
        elif agent == "risk_manager":
            temp = 0.15
            max_tokens = 1500
            timeout = 10
        else:
            temp = 0.1
            max_tokens = 2000
            timeout = 10

        yaml_name = agent_yaml_names.get(agent, agent)

        lines.extend([
            f"  {yaml_name}:",
            f"    provider_type: 'ollama_cloud'",
            f"    model_name: '{model}'  # Score: {score:.0f}/100",
            f"    temperature: {temp}",
            f"    max_tokens: {max_tokens}",
            f"    timeout: {timeout}",
            f"    api_base: 'http://localhost:11434'",
        ])

        if agent == "visual":
            lines.append(f"    supports_vision: true")

        lines.append("")

    return "\n".join(lines)


def print_analysis(recommendations: dict):
    """Imprime an치lisis detallado."""
    print("\n" + "="*90)
    print("游늵 AN츼LISIS COMPARATIVO DE MODELOS - RECOMENDACI칍N PARA TIMEFRAME 1min")
    print("="*90)

    print("\n" + "-"*90)
    print("RESUMEN DE MODELOS DISPONIBLES:")
    print("-"*90)
    print(f"{'Modelo':<35} {'Params':<10} {'Latencia':<12} {'Score 1min':<12} {'Especializaci칩n'}")
    print("-"*90)

    for name, profile in sorted(MODEL_PROFILES.items(), key=lambda x: x[1].score_1min_tf, reverse=True):
        spec = ", ".join(profile.specialization[:2])
        print(f"{name:<35} {profile.params_b:<10.0f}B {profile.expected_latency_1k_tokens:<12.0f}ms "
              f"{profile.score_1min_tf:<12.0f} {spec}")

    print("\n" + "="*90)
    print("RECOMENDACIONES POR AGENTE:")
    print("="*90)

    for agent, rec in recommendations.items():
        print(f"\n游늷 {agent.upper()}")
        print(f"   Descripci칩n: {rec['description']}")
        print(f"   Requisitos: {', '.join(rec['requirements'])}")
        print()
        print(f"   {'Rank':<6} {'Modelo':<35} {'Score':<10} {'Latencia':<12}")
        print(f"   {'-'*65}")

        for i, model_rec in enumerate(rec["top_3_models"][:3], 1):
            medal = "游볞" if i == 1 else "游볟" if i == 2 else "游볠"
            print(f"   {medal} {i:<4} {model_rec['model']:<35} {model_rec['score']:<10.0f} "
                  f"{model_rec['expected_latency_ms']:<12.0f}ms")

    print("\n" + "="*90)


def main():
    """Funci칩n principal."""
    print("\n" + "="*90)
    print("游댧 AN츼LISIS TE칍RICO DE MODELOS LLM PARA FENIX TRADING BOT")
    print("="*90)
    print("\nObjetivo: Optimizar asignaci칩n de modelos para timeframe 1min")
    print("Criterio: Velocidad (40%) + Especializaci칩n (30%) + Score 1min (20%) + Throughput (10%)")

    # Generar recomendaciones
    recommendations = generate_recommendations()

    # Imprimir an치lisis
    print_analysis(recommendations)

    # Generar YAML
    yaml_config = generate_yaml_config(recommendations)

    print("\n" + "="*90)
    print("丘뙖잺 CONFIGURACI칍N YAML RECOMENDADA:")
    print("="*90)
    print(yaml_config)

    # Guardar a archivo
    output_path = "config/llm_providers_1min_optimized.yaml"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(yaml_config)

    print(f"\n游 Configuraci칩n guardada en: {output_path}")

    # Guardar JSON detallado
    json_path = "logs/model_analysis_recommendation.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "timeframe": "1m",
            "recommendations": recommendations,
            "model_profiles": {k: {
                "name": v.name,
                "params_b": v.params_b,
                "architecture": v.architecture,
                "specialization": v.specialization,
                "expected_latency_1k_tokens": v.expected_latency_1k_tokens,
                "throughput_tokens_per_sec": v.throughput_tokens_per_sec,
                "score_1min_tf": v.score_1min_tf,
            } for k, v in MODEL_PROFILES.items()},
        }, f, indent=2, ensure_ascii=False)

    print(f"游 An치lisis detallado guardado en: {json_path}")
    print("\n" + "="*90)

    # Consejos finales
    print("\n游늶 CONSEJOS PARA IMPLEMENTACI칍N:")
    print("-"*90)
    print("""
1. PRIORIDAD VELOCIDAD: En timeframe 1min, cada segundo cuenta. Los modelos 8B-14B
   son ideales para agentes paralelos (technical, sentiment, qabba).

2. PIPELINE CR칈TICO: El agente Decision puede usar un modelo m치s pesado (24B-30B)
   ya que se ejecuta despu칠s de los agentes paralelos.

3. VISION ESPECIAL: El agente Visual debe usar gemini-3-flash-preview:cloud que
   es el 칰nico con capacidad de visi칩n en la lista.

4. REINTENTOS: Configurar max_retries=3 con backoff para manejar fallos de
   validaci칩n JSON, especialmente con modelos m치s r치pidos.

5. VALIDACI칍N: El sistema de validaci칩n autom치tica es crucial - descarta
   respuestas que no cumplan el formato JSON estricto.

6. MONITOREO: Usar el ProductionModelRotator para recolectar m칠tricas reales
   y ajustar la asignaci칩n seg칰n comportamiento en producci칩n.

7. FALLBACKS: Configurar siempre modelos fallback m치s ligeros por si el
   modelo principal falla o est치 sobrecargado.
""")
    print("="*90)


if __name__ == "__main__":
    main()
